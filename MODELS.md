# ğŸ¤– Modelos Compatibles

Este agente funciona con **cualquier modelo de Ollama**. No necesitas usar un modelo especÃ­fico.

## âœ… Auto-detecciÃ³n de Modelos

El agente **detecta automÃ¡ticamente** el modelo que tienes instalado. No necesitas configurar nada.

### Ejemplo con tu modelo actual (gemma2:4b)

```bash
# Ya tienes instalado gemma2:4b, perfecto!
$ python run_agent.py "Â¿QuÃ© es Python?"

â„¹ï¸  Usando modelo: gemma2:4b
ğŸ” Buscando informaciÃ³n sobre: Â¿QuÃ© es Python?
ğŸ’­ Generando respuesta...
...
```

El agente usa automÃ¡ticamente `gemma2:4b` porque es el que detecta instalado.

## ğŸ¯ Modelos Recomendados

Todos estos modelos funcionan perfectamente:

### RÃ¡pidos y Eficientes
- **gemma2:4b** âœ… â† El que TÃš tienes (Â¡excelente elecciÃ³n!)
- **gemma2:2b** - MÃ¡s pequeÃ±o, mÃ¡s rÃ¡pido
- **phi3** (2.3GB) - De Microsoft, muy eficiente
- **llama3.2:1b** (1.3GB) - El mÃ¡s pequeÃ±o y rÃ¡pido

### Balanceados (Recomendados)
- **llama3.2** (2.7GB) - Muy popular, buen balance
- **mistral** (4.1GB) - Excelente calidad

### MÃ¡s Potentes (Requieren mÃ¡s RAM)
- **llama3.1:8b** (8GB) - Muy potente
- **mixtral** (24GB) - Modelo grande, mÃ¡xima calidad

## ğŸ”„ Cambiar de Modelo

Si quieres usar un modelo diferente, tienes dos opciones:

### OpciÃ³n 1: El agente usa el primero que encuentra

```bash
# El agente usa automÃ¡ticamente el primer modelo disponible
$ ollama list
NAME            SIZE
gemma2:4b       2.5 GB  â† UsarÃ¡ este
phi3            2.3 GB
```

### OpciÃ³n 2: Especificar el modelo manualmente

En tu cÃ³digo Python:

```python
from agent import ResearchAgent

# Usar un modelo especÃ­fico
agent = ResearchAgent(model="phi3")  # Usa phi3 en lugar de gemma2

respuesta = agent.research("Tu pregunta")
```

## ğŸ“¥ Instalar MÃ¡s Modelos

Puedes tener mÃºltiples modelos instalados:

```bash
# Ver modelos disponibles en Ollama
ollama list

# Instalar un modelo adicional
ollama pull llama3.2

# Ahora tienes dos modelos
ollama list
# gemma2:4b
# llama3.2

# El agente usarÃ¡ gemma2:4b (el primero)
# Pero puedes especificar llama3.2 si quieres
```

## ğŸ†š ComparaciÃ³n de Modelos

| Modelo | TamaÃ±o | Velocidad | Calidad | RAM MÃ­nima |
|--------|--------|-----------|---------|------------|
| gemma2:2b | 1.6GB | â­â­â­â­â­ | â­â­â­ | 4GB |
| **gemma2:4b** | 2.5GB | â­â­â­â­ | â­â­â­â­ | 6GB |
| phi3 | 2.3GB | â­â­â­â­ | â­â­â­â­ | 6GB |
| llama3.2:1b | 1.3GB | â­â­â­â­â­ | â­â­â­ | 4GB |
| llama3.2 | 2.7GB | â­â­â­â­ | â­â­â­â­ | 8GB |
| mistral | 4.1GB | â­â­â­ | â­â­â­â­â­ | 8GB |

**Tu modelo actual (gemma2:4b) es una excelente elecciÃ³n:**
- âœ… TamaÃ±o moderado (2.5GB)
- âœ… Buena velocidad
- âœ… Muy buena calidad
- âœ… De Google (bien mantenido)

## ğŸ’¡ RecomendaciÃ³n

**No necesitas cambiar nada.** Tu `gemma2:4b` funciona perfectamente. 

Solo instalarÃ­as otro modelo si:
- Quieres uno mÃ¡s rÃ¡pido (gemma2:2b, llama3.2:1b)
- Quieres mejor calidad (mistral)
- Tienes problemas de RAM (usa uno mÃ¡s pequeÃ±o)

## ğŸ” Ver QuÃ© Modelo EstÃ¡ Usando

```bash
# El agente te dice quÃ© modelo usa
$ python run_agent.py "test"

â„¹ï¸  Usando modelo: gemma2:4b  â† Te lo muestra aquÃ­
ğŸ” Buscando informaciÃ³n...
```

## â“ FAQ

**P: Â¿Tengo que instalar llama3.2?**  
R: No. El agente funciona con cualquier modelo, incluyendo tu gemma2:4b.

**P: Â¿Puedo usar varios modelos?**  
R: SÃ­. Instala varios y especifica cuÃ¡l usar:
```python
agent = ResearchAgent(model="llama3.2")
```

**P: Â¿CuÃ¡l es mejor?**  
R: Depende de tu uso:
- Para velocidad: gemma2:2b, llama3.2:1b
- Para calidad: mistral
- Balance (tu caso): gemma2:4b â† Â¡Perfecto!

**P: Â¿Mi modelo es lo suficientemente bueno?**  
R: SÃ­. gemma2:4b es excelente para uso general. Solo necesitarÃ­as cambiar si tienes necesidades muy especÃ­ficas.

## âœ… Resumen

- âœ… No necesitas cambiar nada
- âœ… gemma2:4b funciona perfectamente
- âœ… El agente lo detecta automÃ¡ticamente
- âœ… Puedes instalar otros modelos si quieres experimentar

Â¡Tu configuraciÃ³n actual es perfecta para empezar! ğŸ‰
